{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Barrier Comparison: CE vs ML Potentials\n",
    "\n",
    "This notebook visualizes the comparison between Cluster Expansion (CE) and ML potential (CHGNet/MACE) energy barriers.\n",
    "\n",
    "**Separate analysis for TRAIN and TEST sets to evaluate generalization.**\n",
    "\n",
    "Run `5_compare_energy_barriers.py` first to generate the data:\n",
    "```bash\n",
    "python 5_compare_energy_barriers.py --model chgnet --n_samples 500 --gpu_idx 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which model results to load\n",
    "model_type = 'mace_medium_omat_0'  # or 'mace_medium_omat_0'\n",
    "\n",
    "results_dir = Path('./comparison_results')\n",
    "json_file = results_dir / f'barrier_comparison_{model_type}.json'\n",
    "npz_file = results_dir / f'barrier_data_{model_type}.npz'\n",
    "\n",
    "# Load JSON for detailed results\n",
    "with open(json_file, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Load numpy arrays\n",
    "data = np.load(npz_file)\n",
    "\n",
    "# Train data\n",
    "train_ce = data['train_ce_barriers']\n",
    "train_ml = data['train_ml_barriers']\n",
    "train_errors = data['train_errors']\n",
    "\n",
    "# Test data\n",
    "test_ce = data['test_ce_barriers']\n",
    "test_ml = data['test_ml_barriers']\n",
    "test_errors = data['test_errors']\n",
    "\n",
    "print(f\"Loaded results for {model_type}\")\n",
    "print(f\"  Train barriers: {len(train_ce)}\")\n",
    "print(f\"  Test barriers:  {len(test_ce)}\")\n",
    "print(f\"\\nConfig:\")\n",
    "for k, v in results['config'].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics Summary: TRAIN vs TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = results['train_statistics']\n",
    "test_stats = results['test_statistics']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: TRAIN vs TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'TRAIN':<20} {'TEST':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'N barriers':<25} {train_stats['n_barriers']:<20} {test_stats['n_barriers']:<20}\")\n",
    "print(f\"{'RMSE (meV)':<25} {train_stats['rmse_eV']*1000:<20.2f} {test_stats['rmse_eV']*1000:<20.2f}\")\n",
    "print(f\"{'MAE (meV)':<25} {train_stats['mae_eV']*1000:<20.2f} {test_stats['mae_eV']*1000:<20.2f}\")\n",
    "print(f\"{'Max error (meV)':<25} {train_stats['max_error_eV']*1000:<20.2f} {test_stats['max_error_eV']*1000:<20.2f}\")\n",
    "print(f\"{'Pearson R':<25} {train_stats['pearson_correlation']:<20.4f} {test_stats['pearson_correlation']:<20.4f}\")\n",
    "print(f\"{'Spearman R':<25} {train_stats['spearman_correlation']:<20.4f} {test_stats['spearman_correlation']:<20.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generalization check\n",
    "rmse_ratio = test_stats['rmse_eV'] / train_stats['rmse_eV']\n",
    "print(f\"\\nGeneralization (TEST/TRAIN RMSE ratio): {rmse_ratio:.2f}\")\n",
    "if rmse_ratio < 1.2:\n",
    "    print(\"  Good generalization (ratio < 1.2)\")\n",
    "elif rmse_ratio < 1.5:\n",
    "    print(\"  Moderate overfitting (1.2 < ratio < 1.5)\")\n",
    "else:\n",
    "    print(\"  Significant overfitting (ratio > 1.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parity Plots: TRAIN vs TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "datasets = [\n",
    "    ('TRAIN', train_ml, train_ce, train_stats, 'steelblue'),\n",
    "    ('TEST', test_ml, test_ce, test_stats, 'coral')\n",
    "]\n",
    "\n",
    "# Get common limits\n",
    "all_ml = np.concatenate([train_ml, test_ml])\n",
    "all_ce = np.concatenate([train_ce, test_ce])\n",
    "lims = [min(all_ml.min(), all_ce.min()) - 0.5,\n",
    "        max(all_ml.max(), all_ce.max()) + 0.5]\n",
    "\n",
    "for ax, (name, ml, ce, stats, color) in zip(axes, datasets):\n",
    "    ax.scatter(ml, ce, alpha=0.5, s=30, c=color, edgecolors='none')\n",
    "    \n",
    "    # Parity line\n",
    "    ax.plot(lims, lims, 'k--', lw=2, label='Parity')\n",
    "    \n",
    "    # Linear fit\n",
    "    coeffs = np.polyfit(ml, ce, 1)\n",
    "    fit_line = np.poly1d(coeffs)\n",
    "    x_fit = np.linspace(lims[0], lims[1], 100)\n",
    "    ax.plot(x_fit, fit_line(x_fit), 'r-', lw=2, \n",
    "            label=f'Fit: y = {coeffs[0]:.3f}x + {coeffs[1]:.3f}')\n",
    "    \n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_xlabel('ML Barrier (eV)', fontsize=14)\n",
    "    ax.set_ylabel('CE Barrier (eV)', fontsize=14)\n",
    "    ax.set_title(f'{name}\\nR² = {stats[\"pearson_correlation\"]**2:.4f}, '\n",
    "                 f'RMSE = {stats[\"rmse_eV\"]*1000:.1f} meV', fontsize=14)\n",
    "    ax.legend(loc='upper left', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'parity_plot_train_test_{model_type}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Distribution: TRAIN vs TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Combined histogram\n",
    "ax = axes[0]\n",
    "bins = np.linspace(\n",
    "    min(train_errors.min(), test_errors.min()) * 1000 - 50,\n",
    "    max(train_errors.max(), test_errors.max()) * 1000 + 50,\n",
    "    40\n",
    ")\n",
    "\n",
    "ax.hist(train_errors * 1000, bins=bins, alpha=0.6, label='TRAIN', color='steelblue', edgecolor='black')\n",
    "ax.hist(test_errors * 1000, bins=bins, alpha=0.6, label='TEST', color='coral', edgecolor='black')\n",
    "ax.axvline(0, color='black', linestyle='--', lw=2)\n",
    "ax.axvline(np.mean(train_errors) * 1000, color='steelblue', linestyle='-', lw=2, \n",
    "           label=f'Train mean: {np.mean(train_errors)*1000:.1f} meV')\n",
    "ax.axvline(np.mean(test_errors) * 1000, color='coral', linestyle='-', lw=2,\n",
    "           label=f'Test mean: {np.mean(test_errors)*1000:.1f} meV')\n",
    "ax.set_xlabel('Error (CE - ML) [meV]', fontsize=14)\n",
    "ax.set_ylabel('Count', fontsize=14)\n",
    "ax.set_title('Error Distribution: TRAIN vs TEST', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "ax = axes[1]\n",
    "bp = ax.boxplot([train_errors * 1000, test_errors * 1000], \n",
    "                labels=['TRAIN', 'TEST'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][1].set_facecolor('coral')\n",
    "ax.axhline(0, color='black', linestyle='--', lw=2)\n",
    "ax.set_ylabel('Error (CE - ML) [meV]', fontsize=14)\n",
    "ax.set_title('Error Distribution Comparison', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'error_distribution_train_test_{model_type}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error by Element Type (Train vs Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results by dataset and hopping element\n",
    "def group_by_element(barriers_list):\n",
    "    by_element = {}\n",
    "    for item in barriers_list:\n",
    "        elem = item['atom_symbol']\n",
    "        if elem not in by_element:\n",
    "            by_element[elem] = {'ce': [], 'ml': [], 'error': []}\n",
    "        by_element[elem]['ce'].append(item['ce_barrier'])\n",
    "        by_element[elem]['ml'].append(item['ml_barrier'])\n",
    "        by_element[elem]['error'].append(item['error'])\n",
    "    return by_element\n",
    "\n",
    "train_barriers = [b for b in results['barriers'] if b['dataset'] == 'train']\n",
    "test_barriers = [b for b in results['barriers'] if b['dataset'] == 'test']\n",
    "\n",
    "train_by_elem = group_by_element(train_barriers)\n",
    "test_by_elem = group_by_element(test_barriers)\n",
    "\n",
    "# Print statistics by element\n",
    "print(\"TRAIN - Statistics by hopping element:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Element':<10} {'Count':<8} {'MAE (meV)':<12} {'RMSE (meV)':<12} {'Spearman':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "elements = sorted(set(train_by_elem.keys()) | set(test_by_elem.keys()))\n",
    "for elem in elements:\n",
    "    if elem in train_by_elem:\n",
    "        ce = np.array(train_by_elem[elem]['ce'])\n",
    "        ml = np.array(train_by_elem[elem]['ml'])\n",
    "        err = np.array(train_by_elem[elem]['error'])\n",
    "        mae = np.mean(np.abs(err)) * 1000\n",
    "        rmse = np.sqrt(np.mean(err**2)) * 1000\n",
    "        sp, _ = spearmanr(ce, ml) if len(ce) > 2 else (0, 1)\n",
    "        print(f\"{elem:<10} {len(ce):<8} {mae:<12.2f} {rmse:<12.2f} {sp:<10.4f}\")\n",
    "\n",
    "print(\"\\nTEST - Statistics by hopping element:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Element':<10} {'Count':<8} {'MAE (meV)':<12} {'RMSE (meV)':<12} {'Spearman':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for elem in elements:\n",
    "    if elem in test_by_elem:\n",
    "        ce = np.array(test_by_elem[elem]['ce'])\n",
    "        ml = np.array(test_by_elem[elem]['ml'])\n",
    "        err = np.array(test_by_elem[elem]['error'])\n",
    "        mae = np.mean(np.abs(err)) * 1000\n",
    "        rmse = np.sqrt(np.mean(err**2)) * 1000\n",
    "        sp, _ = spearmanr(ce, ml) if len(ce) > 2 else (0, 1)\n",
    "        print(f\"{elem:<10} {len(ce):<8} {mae:<12.2f} {rmse:<12.2f} {sp:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot by element for Train and Test\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (name, by_elem, color) in zip(axes, [('TRAIN', train_by_elem, 'steelblue'), \n",
    "                                              ('TEST', test_by_elem, 'coral')]):\n",
    "    elements_present = sorted(by_elem.keys())\n",
    "    error_data = [np.array(by_elem[elem]['error']) * 1000 for elem in elements_present]\n",
    "    \n",
    "    if len(error_data) > 0:\n",
    "        bp = ax.boxplot(error_data, labels=elements_present, patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor(color)\n",
    "        ax.axhline(0, color='black', linestyle='--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel('Hopping Element', fontsize=14)\n",
    "    ax.set_ylabel('Error (CE - ML) [meV]', fontsize=14)\n",
    "    ax.set_title(f'{name}: Error by Element', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'error_by_element_train_test_{model_type}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cumulative Error: TRAIN vs TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for name, errors, color in [('TRAIN', train_errors, 'steelblue'), \n",
    "                             ('TEST', test_errors, 'coral')]:\n",
    "    abs_errors = np.abs(errors) * 1000\n",
    "    sorted_errors = np.sort(abs_errors)\n",
    "    cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cumulative * 100, lw=2, color=color, label=name)\n",
    "\n",
    "# Add reference lines\n",
    "for threshold in [50, 100, 200]:\n",
    "    ax.axvline(threshold, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Absolute Error |CE - ML| [meV]', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Percentage (%)', fontsize=14)\n",
    "ax.set_title('Cumulative Error Distribution: TRAIN vs TEST', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, max(np.abs(train_errors).max(), np.abs(test_errors).max()) * 1000 * 1.1)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'cumulative_error_train_test_{model_type}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print percentages\n",
    "print(\"Percentage of samples within error threshold:\")\n",
    "print(f\"{'Threshold':<15} {'TRAIN':<15} {'TEST':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for threshold in [25, 50, 100, 200, 500]:\n",
    "    train_pct = np.mean(np.abs(train_errors) * 1000 <= threshold) * 100\n",
    "    test_pct = np.mean(np.abs(test_errors) * 1000 <= threshold) * 100\n",
    "    print(f\"{threshold:>4} meV       {train_pct:>5.1f}%          {test_pct:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel comparison: CE vs {model_type.upper()}\")\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  TRAIN: {train_stats['n_barriers']} barriers\")\n",
    "print(f\"  TEST:  {test_stats['n_barriers']} barriers\")\n",
    "\n",
    "print(f\"\\n{'='*35}\")\n",
    "print(f\"{'Metric':<20} {'TRAIN':<10} {'TEST':<10}\")\n",
    "print(f\"{'='*35}\")\n",
    "print(f\"{'RMSE (meV)':<20} {train_stats['rmse_eV']*1000:<10.1f} {test_stats['rmse_eV']*1000:<10.1f}\")\n",
    "print(f\"{'MAE (meV)':<20} {train_stats['mae_eV']*1000:<10.1f} {test_stats['mae_eV']*1000:<10.1f}\")\n",
    "print(f\"{'R²':<20} {train_stats['pearson_correlation']**2:<10.4f} {test_stats['pearson_correlation']**2:<10.4f}\")\n",
    "print(f\"{'Spearman R':<20} {train_stats['spearman_correlation']:<10.4f} {test_stats['spearman_correlation']:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION FOR KMC APPLICATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check generalization\n",
    "rmse_ratio = test_stats['rmse_eV'] / train_stats['rmse_eV']\n",
    "print(f\"\\nGeneralization (TEST/TRAIN RMSE): {rmse_ratio:.2f}x\")\n",
    "if rmse_ratio < 1.2:\n",
    "    print(\"  Good generalization\")\n",
    "elif rmse_ratio < 1.5:\n",
    "    print(\"  Moderate overfitting - consider more regularization\")\n",
    "else:\n",
    "    print(\"  Significant overfitting - need more training data or regularization\")\n",
    "\n",
    "# Check ranking ability (important for KMC)\n",
    "print(f\"\\nRanking ability (Spearman on TEST): {test_stats['spearman_correlation']:.4f}\")\n",
    "if test_stats['spearman_correlation'] > 0.9:\n",
    "    print(\"  Excellent - CE will rank barriers correctly for KMC\")\n",
    "elif test_stats['spearman_correlation'] > 0.7:\n",
    "    print(\"  Good - CE should capture main kinetic trends\")\n",
    "else:\n",
    "    print(\"  Poor - CE may miss important kinetic details\")\n",
    "\n",
    "# Check absolute accuracy\n",
    "print(f\"\\nAbsolute accuracy (TEST RMSE): {test_stats['rmse_eV']*1000:.1f} meV\")\n",
    "if test_stats['rmse_eV'] < 0.05:\n",
    "    print(\"  Excellent - quantitative accuracy for barrier heights\")\n",
    "elif test_stats['rmse_eV'] < 0.1:\n",
    "    print(\"  Good - semi-quantitative accuracy\")\n",
    "elif test_stats['rmse_eV'] < 0.3:\n",
    "    print(\"  Moderate - qualitative trends should be correct\")\n",
    "else:\n",
    "    print(\"  Poor - consider improving CE fitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
